// This autogenerated skeleton file illustrates how to build a server.
// You should copy it to another filename to avoid overwriting it.

#include "NodeHandler.h"

#include "Hub.h"
#include "Node.h"
#include "HubHandler.h"
#include "settings.h"
#include <thrift/protocol/TBinaryProtocol.h>
#include <thrift/server/TSimpleServer.h>
#include <thrift/transport/TServerSocket.h>
#include <thrift/transport/TBufferTransports.h>
#include <thrift/concurrency/ThreadManager.h>
#include <thrift/server/TNonblockingServer.h>
#include <boost/thread.hpp>
#include "NodeClient.h"
#include "HubClient.h"

#include <cstdlib>
#include <csignal>
#include <ctime>

using namespace apache::thrift;
using namespace apache::thrift::protocol;
using namespace apache::thrift::transport;
using namespace apache::thrift::server;
using namespace apache::thrift::concurrency;

using boost::shared_ptr;

using namespace mp3;

NodeHandler::NodeHandler() : state(FREE), batch_size(3), running(true), lamportTimestamp(0) {
  srand (time(NULL));
  myPort = rand() % 10000 + 20000;
}
NodeHandler::~NodeHandler() {}

int NodeHandler::getPort(){
  return myPort;
}
void NodeHandler::joinNotify(const int16_t port){
  boost::unique_lock<boost::mutex> lock(nodeMapMutex);
  DEBUG("Notified of " << port << " join.");
//  nodeMap.insert(std::make_pair(port, boost::shared_ptr<NodeIf>(new SimpleNodeClient(port))));
  nodeMap.insert(std::make_pair(port, boost::shared_ptr<NodeIf>(new SimpleNodeClient(port))));

  gotReplyCondition.notify_all();
}

void NodeHandler::leaveNotify(const int16_t port){
  boost::unique_lock<boost::mutex> lock(nodeMapMutex);
  DEBUG("Notified of " << port << " leaving.");
  nodeMap.erase(port);
  gotReplyCondition.notify_all();
}

int16_t NodeHandler::getNumJobs() {
  return 0;
}

void NodeHandler::queueJob(const Job& job) {
  boost::unique_lock<boost::mutex> lock(queueMutex);
  DEBUG("Queued job " << job.id);

  queue.push_back( job );
  queueCondition.notify_one();
}

void NodeHandler::populateQueue(){
  // Enter local mutex
  boost::unique_lock<boost::mutex> lock(gotReplyMutex);
  boost::unique_lock<boost::mutex> lock2(nodeMapMutex);

  // Enter distributed mutex
  DEBUG("Acquiring distributed mutex...");
  state = WANTED;
  numReplied = 0;

  // Send out all the requests
  lamportTimestamp++;
  for(boost::unordered_map<int, boost::shared_ptr<NodeIf> >::iterator iterator = nodeMap.begin(); iterator != nodeMap.end(); iterator++) {
    iterator->second->notifyMutexRequest(lamportTimestamp, myPort);
  }

  // Wait until you get a reply from each node
  DEBUG("Waiting until I hear responses from all " << nodeMap.size() << " folks.");
  while(numReplied < nodeMap.size()) gotReplyCondition.wait(lock);

  DEBUG("Got mutex!");
  state = HELD;

    // Find out who has what.
    DEBUG("Don't have enough jobs. Who has some extras?");
    std::vector< Job > newJobs;
    boost::unordered_map< int, int > nums;
    for(boost::unordered_map<int, boost::shared_ptr<NodeIf> >::iterator iterator = nodeMap.begin(); iterator != nodeMap.end(); iterator++) {
      nums[iterator->first] = iterator->second->getNumExtraJobs();
      DEBUG(iterator->first << " has " << nums[iterator->first] << " extra.");
    }

    // Get jobs from people.
    for(boost::unordered_map<int, int >::iterator iterator = nums.begin(); iterator != nums.end(); iterator++) {
      int have = iterator->second;
      int need = batch_size - (queue.size() + newJobs.size());

      if(have > 0 && need > 0){
        int m;
        if(have > need){
          m = need;
        }
        else{
          m = have;
        }

        DEBUG("Getting " << m << " jobs from " << iterator->first << ".");
        std::vector< Job > v;
        nodeMap[iterator->first]->giveJobs(v, m);
        newJobs.insert(newJobs.end(),v.begin(),v.end());
      }
    }

    DEBUG("Updating the queue.");
    // Update queue
    queue.insert(queue.end(),newJobs.begin(),newJobs.end());

  // Exit mutex
  DEBUG("Exiting the mutex.");
  boost::unique_lock<boost::mutex> lock3(requestQueueMutex);
  state = FREE;
  // Notify everyone
  for(int i = 0; i < requestQueue.size(); i++){
    DEBUG("Replying to " << requestQueue[i].second << ".");
    nodeMap[requestQueue[i].second]->notifyMutexReply(myPort);
  }
  requestQueue.clear();
  DEBUG("Done exiting mutex.");
}

void NodeHandler::processJobs(std::vector<Result> & _return) {
  boost::unique_lock<boost::mutex> lock(queueMutex);

  if(!running){
    MyError e;
    e.message = string("problem1");
    throw e;
  }

  if(queue.size() < batch_size){
    populateQueue();
  }

  while(queue.size()==0 && running) queueCondition.wait(lock);
  if(!running){
    MyError e;
    e.message = string("problem2");
    throw e;
  }
  DEBUG("Processing min(" << batch_size << "," << queue.size() << ") jobs:");

  // Make a batch from the queue
  std::vector<Job> batch;
  int count = 0;

  // The hopped jobs get priority
  int sz = queue.size();
  for(int i = 0; i < sz; i++){
    if(count == batch_size){
      break;
    }
    else if(queue[i-count].hopcount > 0) {
      DEBUG("Processing hopped job " << queue[i-count].id << ".");
      batch.push_back(queue[i-count]);
      queue.erase(queue.begin() + i - count);
      count++;
    }
  }

  sz = queue.size();
  for(int i = 0; i < sz; i++){
    if(count == batch_size){
      break;
    }
    else{
      DEBUG("Processing job " << queue[i-count].id << ".");
      batch.push_back(queue[i-count]);
      queue.erase(queue.begin() + i - count);
      count++;
    }
  }

  // Return result
  for(int i = 0; i < batch.size(); i++){
    Result r;
    r.job = batch[i].id;
    _return.push_back(r);
  }
}

int16_t NodeHandler::getNumExtraJobs() {
  return queue.size() - batch_size;
}

void NodeHandler::notifyMutexRequest(const int16_t lamportTs, const int16_t fromPort) {
  boost::unique_lock<boost::mutex> lock(requestQueueMutex);
  lamportTimestamp++;

  if(state == FREE || (state == WANTED && (lamportTs < lamportTimestamp || (lamportTs == lamportTimestamp && fromPort < myPort))))
  {
    DEBUG("Replied to request from " << fromPort << " with timestamp " << lamportTs);
    nodeMap[fromPort]->notifyMutexReply(myPort);
  }
  else{
    DEBUG("Queued request from " << fromPort << " with timestamp " << lamportTs);
    requestQueue.push_back(make_pair(lamportTs, fromPort));
  }

  if(lamportTs > lamportTimestamp){
    lamportTimestamp = lamportTs;
  }
}

void NodeHandler::notifyMutexReply(const int16_t fromPort) {
  boost::unique_lock<boost::mutex> lock(gotReplyMutex);
  DEBUG("Got reply from " << fromPort);
  lamportTimestamp++;
  numReplied++;
  gotReplyCondition.notify_all();
}

void NodeHandler::giveJobs(std::vector< Job > & _return, const int16_t numJobs) {
  boost::unique_lock<boost::mutex> lock(queueMutex);

  int sz = queue.size();
  int count = 0;
  for(int i = 0; i < sz; i++){
    if(count == numJobs){
      break;
    }
    else if(queue[i-count].hopcount == 0){
      DEBUG("Transfering job " << queue[i-count].id << ".");
      queue[i-count].hopcount++;
      _return.push_back(queue[i-count]);
      queue.erase(queue.begin() + i - count);
      count++;
    }
  }
  if(count < numJobs){
    DEBUG("Sorry bro. Ran out of jobs.");
  }
}

void NodeHandler::setBatchSize(const int16_t size){
  boost::unique_lock<boost::mutex> lock(queueMutex);
  batch_size = size;
  DEBUG("Updating batch size to " << size);
  populateQueue();
  queueCondition.notify_one();
}

void NodeHandler::shutdown(void){
  running = false;
  queueCondition.notify_all();
  DEBUG("Shutting down server.");
}















namespace mp3{
class ServerProxy{
private:
shared_ptr<TNonblockingServer> server;
public:
ServerProxy(shared_ptr<TNonblockingServer> srv);
~ServerProxy();
void operator () ();
};

class JobConsumer{
  private:
    shared_ptr<NodeIf> node;
  public:
    JobConsumer(shared_ptr<NodeIf> nd);
    ~JobConsumer();
    void operator () ();
};

class BatchSizeAllocator{
  private:
    shared_ptr<NodeIf> node;
  public:
    BatchSizeAllocator(shared_ptr<NodeIf> nd);
    ~BatchSizeAllocator();
    void operator () ();
};

}

ServerProxy::ServerProxy(shared_ptr<TNonblockingServer> srv) : server(srv){}
ServerProxy::~ServerProxy(){}
void ServerProxy::operator () (){
  DEBUG("Starting server.");
  server->serve();
}

JobConsumer::JobConsumer(shared_ptr<NodeIf> nd) : node(nd){}
JobConsumer::~JobConsumer(){}
void JobConsumer::operator () (){
  DEBUG("Job consumer started...");
  try{
  while(true){
    std::vector<Result> rslt;
    DEBUG("Job consumer acquiring lock...");
    node->processJobs(rslt);
    DEBUG("Job consumer processing...");
    boost::this_thread::sleep(boost::posix_time::milliseconds(rand() % 1000 + 6000));
  }} catch(MyError e){
    DEBUG("Exiting processor.");
  }
}

BatchSizeAllocator::BatchSizeAllocator(shared_ptr<NodeIf> nd) : node(nd){}
BatchSizeAllocator::~BatchSizeAllocator(){}
void BatchSizeAllocator::operator () (){
  DEBUG("Batch size allocator started...");
  while(true){
    int batch_size = 3;
    DEBUG("BSA acquiring lock...");
    node->setBatchSize(batch_size);
    DEBUG("BSA allocated batch size of " << batch_size);
    boost::this_thread::sleep(boost::posix_time::milliseconds(rand() % 1000 + 3000));
  }
}

std::vector< shared_ptr<TNonblockingServer> > running;
boost::thread_group otherThreads;
shared_ptr<NodeHandler> handler(new NodeHandler());

void handle_sig(int signum){
  DEBUG("Caught " << signum);
  handler->shutdown();
  running[0]->stop();
  boost::this_thread::sleep(boost::posix_time::milliseconds(100));
  DEBUG("Interrupting threads.");
  otherThreads.interrupt_all();
}


int main(int argc, char **argv) {
	signal(SIGINT, handle_sig);
	signal(SIGTERM, handle_sig);

  int port = handler->getPort();
  // Start the server.
  shared_ptr<TProcessor> processor(new NodeProcessor(handler));
  shared_ptr<TProtocolFactory> protocolFactory(new TBinaryProtocolFactory());

  shared_ptr<ThreadManager> threadManager = ThreadManager::newSimpleThreadManager(15);
  shared_ptr<PosixThreadFactory> threadFactory = shared_ptr<PosixThreadFactory>(new PosixThreadFactory());
  threadManager->threadFactory(threadFactory);
  threadManager->start();
  shared_ptr<TNonblockingServer> server(new  TNonblockingServer(processor, protocolFactory, port, threadManager));

  // Start the worker thread.
  running.push_back(server);

  ServerProxy s(server);
  boost::thread workerThread(s);

  DEBUG("Started the server " << port << ". Now joining the group.");

  // TODO: This is a hack.
  boost::this_thread::sleep(boost::posix_time::milliseconds(100));

  // Join the group.
  boost::shared_ptr<HubIf> client(new SimpleHubClient(9090));
  client->join(port);

  // Start the job consumer
  boost::shared_ptr<NodeIf> jobConsumerClient(new SimpleNodeClient(port));
  JobConsumer jobConsumer(jobConsumerClient);
  otherThreads.create_thread(jobConsumer);

  boost::shared_ptr<NodeIf> bsaClient(new SimpleNodeClient(port));
  BatchSizeAllocator bsa(bsaClient);
  otherThreads.create_thread(bsa);

  DEBUG("Joined the group, and am now serving.");
  workerThread.join();
  DEBUG("Closed server.");
  otherThreads.join_all();
  DEBUG("Closed job consumer.");
  client->leave(port);
  DEBUG("Exiting...");

  return 0;
}
